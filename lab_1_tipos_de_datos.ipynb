{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8teiBZ6dP5g"
   },
   "source": [
    "# Lab 1: Tipos y Tamaños de Datos\n",
    "\n",
    "En este laboratorio, aprenderás sobre los tipos de datos comunes utilizados para almacenar los parámetros de los modelos de aprendizaje automático."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introducción a los Tipos de Datos\n",
    "\n",
    "Antes de sumergirnos en los tipos de datos en PyTorch, es importante entender algunos conceptos básicos:\n",
    "\n",
    "### Bits y Bytes\n",
    "\n",
    "- **Bit**: Es la unidad mínima de información en informática, puede tener un valor de 0 o 1.\n",
    "- **Byte**: Es un conjunto de 8 bits. Por ejemplo, `10101100` es un byte.\n",
    "\n",
    "### Enteros\n",
    "\n",
    "Los enteros pueden ser con signo (signed) o sin signo (unsigned).\n",
    "\n",
    "- **Unsigned (sin signo)**: Solo puede representar valores positivos y cero. Ejemplo: uint8 puede representar valores de 0 a 255.\n",
    "- **Signed (con signo)**: Puede representar valores positivos, negativos y cero. Ejemplo: int8 puede representar valores de -128 a 127.\n",
    "\n",
    "### Punto Flotante\n",
    "\n",
    "Los números de punto flotante se utilizan para representar números fraccionarios. Se almacenan en una forma que incluye una parte entera y una fracción.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ¿Qué es bfloat16?\n",
    "\n",
    "**bfloat16** o **Brain Floating Point 16** es un formato de punto flotante de 16 bits desarrollado por Google para mejorar el rendimiento en tareas de aprendizaje automático y redes neuronales profundas. La principal diferencia entre bfloat16 y otros formatos de punto flotante radica en cómo se distribuyen los bits para representar los números.\n",
    "\n",
    "### Estructura de bfloat16\n",
    "\n",
    "- **Signo**: 1 bit\n",
    "- **Exponente**: 8 bits\n",
    "- **Mantisa (fracción)**: 7 bits\n",
    "\n",
    "### Comparación con fp16\n",
    "\n",
    "**fp16** o **float16** es otro formato de punto flotante de 16 bits utilizado comúnmente para reducir el uso de memoria y aumentar la velocidad de los cálculos. La estructura de fp16 es la siguiente:\n",
    "\n",
    "- **Signo**: 1 bit\n",
    "- **Exponente**: 5 bits\n",
    "- **Mantisa (fracción)**: 10 bits\n",
    "\n",
    "### Diferencias clave entre bfloat16 y fp16\n",
    "\n",
    "1. **Rango de exponente**: bfloat16 tiene el mismo rango de exponente que float32 debido a sus 8 bits para el exponente, mientras que fp16 tiene un rango de exponente más limitado con solo 5 bits.\n",
    "2. **Precisión**: fp16 tiene más bits dedicados a la mantisa (10 bits) en comparación con bfloat16 (7 bits), lo que le da a fp16 mayor precisión para representar números pequeños.\n",
    "3. **Facilidad de conversión**: bfloat16 facilita la conversión de y hacia float32 sin pérdida significativa de rango, lo cual es beneficioso para tareas de aprendizaje automático que requieren precisión en una amplia gama de valores.\n",
    "\n",
    "### ¿Por qué se usa bfloat16 en lugar de fp32?\n",
    "\n",
    "bfloat16 se utiliza en lugar de fp32 principalmente por las siguientes razones:\n",
    "\n",
    "- **Reducción de memoria**: bfloat16 usa la mitad de memoria en comparación con fp32, lo cual es crucial para manejar modelos grandes y conjuntos de datos extensos.\n",
    "- **Mayor velocidad de cómputo**: Los cálculos con bfloat16 pueden ser significativamente más rápidos, ya que muchas unidades de procesamiento (CPUs, GPUs, TPUs) están optimizadas para operar con este formato.\n",
    "- **Mantiene un buen rango de exponente**: A diferencia de fp16, bfloat16 mantiene el mismo rango de exponente que fp32, permitiendo manejar números muy grandes y muy pequeños sin desbordamientos o subdesbordamientos frecuentes.\n",
    "- **Eficiencia energética**: Operar con bfloat16 consume menos energía en comparación con fp32, lo que es importante para grandes centros de datos y aplicaciones en la nube.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tipos de Datos en PyTorch\n",
    "\n",
    "En PyTorch, los tipos de datos son esenciales para definir y manipular los tensores, que son las estructuras de datos fundamentales utilizadas para almacenar los parámetros de los modelos de aprendizaje automático."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "id": "nFC9wgzxBZvz",
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.331220Z",
     "start_time": "2024-05-21T20:19:54.328710Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW7psx41rn0h"
   },
   "source": "### Enteros"
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.335564Z",
     "start_time": "2024-05-21T20:19:54.332463Z"
    }
   },
   "source": [
    "# Información del `entero sin signo de 8 bits`\n",
    "torch.iinfo(torch.uint8)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=0, max=255, dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.340160Z",
     "start_time": "2024-05-21T20:19:54.336888Z"
    }
   },
   "source": [
    "# Información del `entero de 8 bits (con signo)`\n",
    "torch.iinfo(torch.int8)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.343524Z",
     "start_time": "2024-05-21T20:19:54.341496Z"
    }
   },
   "source": "# Ahora tú, muestra la información del dato tipo `entero de 64 bits (con signo)`\n",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.346496Z",
     "start_time": "2024-05-21T20:19:54.344666Z"
    }
   },
   "source": "# Muestra la información del dato tipo `entero de 32 bits (con signo)`\n",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.349643Z",
     "start_time": "2024-05-21T20:19:54.347747Z"
    }
   },
   "source": "# Muestra la información del dato tipo `entero de 16 bits (con signo)`\n",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Puntos Flotantes"
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.355902Z",
     "start_time": "2024-05-21T20:19:54.353318Z"
    }
   },
   "source": [
    "# Por defecto, Python almacena los datos de punto flotante en fp64\n",
    "value = 1/3"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.360125Z",
     "start_time": "2024-05-21T20:19:54.357235Z"
    }
   },
   "source": [
    "format(value, '.60f')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.333333333333333314829616256247390992939472198486328125000000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.364205Z",
     "start_time": "2024-05-21T20:19:54.361326Z"
    }
   },
   "source": [
    "# Punto flotante de 64 bits\n",
    "tensor_fp64 = torch.tensor(value, dtype = torch.float64)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.368270Z",
     "start_time": "2024-05-21T20:19:54.365312Z"
    }
   },
   "source": "print(f\"tensor fp64: {format(tensor_fp64.item(), '.60f')}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor fp64: 0.333333333333333314829616256247390992939472198486328125000000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.371802Z",
     "start_time": "2024-05-21T20:19:54.369410Z"
    }
   },
   "source": [
    "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
    "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
    "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 81,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.376173Z",
     "start_time": "2024-05-21T20:19:54.373114Z"
    }
   },
   "source": [
    "print(f\"tensor fp64: {format(tensor_fp64.item(), '.60f')}\")\n",
    "print(f\"tensor fp32: {format(tensor_fp32.item(), '.60f')}\")\n",
    "print(f\"tensor fp16: {format(tensor_fp16.item(), '.60f')}\")\n",
    "print(f\"tensor bf16: {format(tensor_bf16.item(), '.60f')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor fp64: 0.333333333333333314829616256247390992939472198486328125000000\n",
      "tensor fp32: 0.333333343267440795898437500000000000000000000000000000000000\n",
      "tensor fp16: 0.333251953125000000000000000000000000000000000000000000000000\n",
      "tensor bf16: 0.333984375000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1699482682456,
     "user": {
      "displayName": "Marc Sun",
      "userId": "00829270524676809963"
     },
     "user_tz": 300
    },
    "height": 47,
    "id": "hUukczHrBodt",
    "outputId": "4e75bdf1-25b6-4e0c-9c08-18208e943b10",
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.380608Z",
     "start_time": "2024-05-21T20:19:54.377387Z"
    }
   },
   "source": [
    "# Información del `punto flotante brain de 16 bits`\n",
    "torch.finfo(torch.bfloat16)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.384550Z",
     "start_time": "2024-05-21T20:19:54.381609Z"
    }
   },
   "source": [
    "# Información del `punto flotante de 32 bits`\n",
    "torch.finfo(torch.float32)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.387817Z",
     "start_time": "2024-05-21T20:19:54.385613Z"
    }
   },
   "source": "# Información del `punto flotante de 16 bits`\n",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.391353Z",
     "start_time": "2024-05-21T20:19:54.389272Z"
    }
   },
   "source": "# Información del `punto flotante de 64 bits`\n",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DbtWVVc_jiW"
   },
   "source": "### Reducción de Precisión (Downcasting)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "id": "JHDsqfauBID9",
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.395777Z",
     "start_time": "2024-05-21T20:19:54.392944Z"
    }
   },
   "source": [
    "# Tensor aleatorio de PyTorch: float32, tamaño=1000\n",
    "tensor_fp32 = torch.rand(1000, dtype=torch.float32)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Nota: Al ser aleatorio, los valores que obtendrás serán diferentes del video.\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.404124Z",
     "start_time": "2024-05-21T20:19:54.396968Z"
    }
   },
   "source": [
    "# Primeros 5 elementos del tensor aleatorio\n",
    "tensor_fp32[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6588, 0.2215, 0.7244, 0.9605, 0.4518])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "id": "xXLANbSx_nx4",
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.407440Z",
     "start_time": "2024-05-21T20:19:54.405117Z"
    }
   },
   "source": [
    "# Reducción de precisión del tensor a bfloat16 usando el método \"to\"\n",
    "tensor_fp32_to_bf16 = tensor_fp32.to(dtype=torch.bfloat16)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.412482Z",
     "start_time": "2024-05-21T20:19:54.408725Z"
    }
   },
   "source": [
    "tensor_fp32_to_bf16[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6602, 0.2217, 0.7227, 0.9609, 0.4512], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.415988Z",
     "start_time": "2024-05-21T20:19:54.413504Z"
    }
   },
   "source": [
    "# tensor_fp32 x tensor_fp32\n",
    "m_float32 = torch.dot(tensor_fp32, tensor_fp32)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.420677Z",
     "start_time": "2024-05-21T20:19:54.417157Z"
    }
   },
   "source": [
    "m_float32"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(342.1302)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.424437Z",
     "start_time": "2024-05-21T20:19:54.421925Z"
    }
   },
   "source": [
    "# tensor_fp32_to_bf16 x tensor_fp32_to_bf16\n",
    "m_bfloat16 = torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.429025Z",
     "start_time": "2024-05-21T20:19:54.425482Z"
    }
   },
   "source": [
    "m_bfloat16"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(342., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "- Usarás \"reducción de precisión\" como una forma simple de cuantización en la próxima lección."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2024-05-21T20:19:54.435893Z",
     "start_time": "2024-05-21T20:19:54.434087Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 27
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKuOYgD21Zty8Z1Smilnjc",
   "collapsed_sections": [
    "x_9J8WavkQGl",
    "sulufN1wkK_L",
    "hqPUM0f8oGgf",
    "TgmPMm-ZvdXX",
    "MJuh8aqo9LO-",
    "3bOevU0Ez4KB",
    "LgtYIhSf0Uu0",
    "9DH7-tDDvK5N",
    "TuZhoPK6weuR",
    "WAOeSNraxZeF",
    "UGXo-IljxmNG",
    "_N3G-dX32awT",
    "hW7psx41rn0h",
    "OmcNPw6zlRp7",
    "3_zvORGrnTR8",
    "jh9IuiovrnoF",
    "isDcbkXxxiTf",
    "wM-xkASw1odi",
    "3DbtWVVc_jiW",
    "ViImpV5rAvyp",
    "RdcyknnjBD99",
    "-eYj4UUXCAlJ",
    "MCLe1N4GCSQT",
    "GIY7IrOv_3cD",
    "8HlFKDBKGNG8"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
